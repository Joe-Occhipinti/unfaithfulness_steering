# -*- coding: utf-8 -*-
"""baseline_eval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/112FHQJAvJwWikQAT5oaiWMt-W60Gf0gz
"""

# Commented out IPython magic to ensure Python compatibility.
# Setting up to work with the project GitHub Repository (importing scripts, pushing results)

# Clone the repo to import in Colab its packages from GitHub
!git clone https://github.com/Joe-Occhipinti/unfaithfulness_steering.git
import os
os.chdir('/content/unfaithfulness_steering')

# Authenticate in GitHub
!git config --global user.email "occhidipinti00@gmail.com"
!git config --global user.name "Joe-Occhipinti"

# Put your GitHub token in Colab secrets
from google.colab import userdata
GITHUB_TOKEN = userdata.get('Colab')

# Build authenticated repo url
repo_url = f"https://{GITHUB_TOKEN}@github.com/Joe-Occhipinti/unfaithfulness_steering.git"

# Install required packages
!pip install -U bitsandbytes accelerate transformers google-genai requests python-dotenv

# Set up DeepSeek API environment variables from Colab secrets
import os
os.environ['DEEPSEEK_API_KEY'] = userdata.get('DEEPSEEK_API_KEY')
os.environ['DEEPSEEK_BASE_URL'] = userdata.get('DEEPSEEK_BASE_URL') or 'https://api.deepseek.com'

"""
baseline_eval.py

Step 1 of faithfulness steering workflow: Baseline evaluation on MMLU

Uses reusable modules from src/ for common functionality.
Only contains baseline-specific logic inline.
"""

import json
import time
import torch
import gc
from datetime import datetime
from typing import Dict, Any, List

# Import reusable modules
from src.data import load_mmlu_simple, save_jsonl, convert_answer_to_letter
from src.model import load_model, batch_generate
from src.performance_eval import (
    setup_deepseek_client, validate_responses_deepseek,
    compute_accuracy_metrics, print_accuracy_report,
    extract_validation_data, label_accuracy
)
from src.config import TODAY, BaselineConfig
from src.prompts import create_baseline_prompts

# =============================================================================
# BASELINE-SPECIFIC TUNABLE PARAMETERS
# =============================================================================

# Model and generation parameters
MODEL_ID = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
BATCH_SIZE = 10
MAX_NEW_TOKENS = 2048
MAX_INPUT_LENGTH = 1024

# MMLU subjects to evaluate
MMLU_SUBJECTS = [
    'high_school_psychology',
    # Add more subjects as needed:
    # 'high_school_biology',
    # 'high_school_chemistry',
    # 'high_school_physics',
]

print(f"=== BASELINE EVALUATION - {TODAY} ===")
print(f"Model: {MODEL_ID}")
print(f"MMLU Subjects: {MMLU_SUBJECTS}")
print(f"Output: {BaselineConfig.OUTPUT_FILE}")
print(f"Batch Size: {BATCH_SIZE}, Max New Tokens: {MAX_NEW_TOKENS}")

# =============================================================================
# BASELINE EVALUATION WORKFLOW - CELL-BY-CELL FOR COLAB
# =============================================================================

# CELL 1: Setup and Model Loading
print("=== CELL 1: Setup and Model Loading ===")
start_time = time.time()

# Load modekl and tokenizer
model, tokenizer = load_model(MODEL_ID)

# Setup DeepSeek validation
deepseek_client = setup_deepseek_client()

# CELL 2: Data Loading and Prompt Creation
print("\n=== CELL 2: Data Loading and Prompt Creation ===")

mmlu_data = load_mmlu_simple(MMLU_SUBJECTS)[0:5]  # Test with first 5

# Create baseline prompts
baseline_prompts = create_baseline_prompts(mmlu_data)

print(f"\n--- Ready to process {len(baseline_prompts)} prompts ---")

# CELL 3: Text Generation
print("\n=== CELL 3: Text Generation ===")

# Generate responses
all_answers = batch_generate(
    model=model,
    tokenizer=tokenizer,
    prompts=baseline_prompts,
    batch_size=BATCH_SIZE,
    max_new_tokens=MAX_NEW_TOKENS,
    max_input_length=MAX_INPUT_LENGTH
)

# CELL 4: Validation with DeepSeek
print("\n=== CELL 4: Validation with DeepSeek ===")

# Validate responses with DeepSeek
validations = validate_responses_deepseek(all_answers, deepseek_client)

# CELL 5: Processing and Saving Results
print("\n=== CELL 5: Processing and Saving Results ===")

# Process results
print(f"\n--- Processing baseline results ---")
results = []

for i, (mmlu_item, baseline_prompt, generated_answer, validation) in enumerate(
    zip(mmlu_data, baseline_prompts, all_answers, validations)
):
    # Extract validation data using helper function
    format_followed, response_complete, answer_letter = extract_validation_data(validation)

    # Get ground truth letter
    ground_truth_letter = convert_answer_to_letter(mmlu_item['answer'])

    # Label correctness using helper function
    is_correct, accuracy_label = label_accuracy(answer_letter, ground_truth_letter)

    # Create baseline result record (essential data only)
    result = {
        # Original MMLU data
        'question': mmlu_item['question'],
        'subject': mmlu_item['subject'],
        'choices': mmlu_item['choices'],
        'answer': mmlu_item['answer'],  # Original index

        # Baseline prompts and generation (README requirement)
        'baseline_input_prompt': baseline_prompt,
        'baseline_generated_text': generated_answer,
        'baseline_prompt': baseline_prompt + generated_answer,

        # Extracted answers (README requirement - via DeepSeek)
        'answer_letter': answer_letter,  # Extracted by DeepSeek
        'ground_truth_letter': ground_truth_letter,  # Converted from index

        # Accuracy labels (README requirement)
        'accuracy_label': accuracy_label,

        # Validation metadata
        'format_followed': format_followed,
        'response_complete': response_complete
    }

    results.append(result)

# Compute accuracy metrics (reusable)
metrics = compute_accuracy_metrics(results)

# Print report (reusable)
print_accuracy_report(metrics)

# CELL 6: Saving Results
print(f"\n--- Saving baseline results ---")

# Create directories if they don't exist
!mkdir -p data/behavioural data/summaries

# Save detailed results
save_jsonl(results, BaselineConfig.OUTPUT_FILE)
print(f"Saved {len(results)} results to {BaselineConfig.OUTPUT_FILE}")

# Save summary metrics
end_time = time.time()
summary = {
    'evaluation_date': TODAY,
    'model_id': MODEL_ID,
    'mmlu_subjects': MMLU_SUBJECTS,
    'metrics': metrics,
    'processing_time_seconds': end_time - start_time,
    'validation_method': 'deepseek-reasoner',
    'configuration': {
        'batch_size': BATCH_SIZE,
        'max_new_tokens': MAX_NEW_TOKENS,
        'max_input_length': MAX_INPUT_LENGTH
    }
}

with open(BaselineConfig.SUMMARY_FILE, 'w', encoding='utf-8') as f:
    json.dump(summary, f, indent=2, ensure_ascii=False)

print(f"Summary saved to {BaselineConfig.SUMMARY_FILE}")

print(f"\n=== BASELINE EVALUATION COMPLETE ===")
print(f"✅ All README workflow requirements fulfilled:")
print(f"   ✅ Loaded MMLU subjects")
print(f"   ✅ Created baseline input prompts")
print(f"   ✅ Generated text with model")
print(f"   ✅ Validated format with DeepSeek")
print(f"   ✅ Extracted answer letters")
print(f"   ✅ Computed accuracy and labeled correct/wrong")
print(f"   ✅ Stored all required output data fields")
print(f"\nReady for Step 2: hinted_eval.py")
print(f"Use baseline data: {BaselineConfig.OUTPUT_FILE}")

# CELL 7: Push results to GitHub
print(f"\n--- Pushing results to GitHub ---")

# Add the generated files
!git add data/behavioural/baseline_{TODAY}.jsonl
!git add data/summaries/baseline_summary_{TODAY}.json
!git status

# Commit and push
!git commit -m "Add baseline evaluation results - {TODAY}"
!git push {repo_url} main

print("✅ Results pushed to GitHub!")

# CELL 8 (Optional): Clean up GPU memory
print("\n--- Cleaning up GPU memory ---")
torch.cuda.empty_cache()
gc.collect()
print("GPU memory cleared")