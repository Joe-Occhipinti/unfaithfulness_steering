# -*- coding: utf-8 -*-
"""extract_activations.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/
"""

# Commented out IPython magic to ensure Python compatibility.
# Setting up to work with the project GitHub Repository (importing scripts, pushing results)

# Clone the repo to import in Colab its packages from GitHub
!git clone https://github.com/Joe-Occhipinti/unfaithfulness_steering.git
import os
os.chdir('/content/unfaithfulness_steering')

# Authenticate in GitHub
!git config --global user.email "occhidipinti00@gmail.com"
!git config --global user.name "Joe-Occhipinti"

# Put your GitHub token in Colab secrets
from google.colab import userdata
GITHUB_TOKEN = userdata.get('Colab')

# Build authenticated repo url
repo_url = f"https://{GITHUB_TOKEN}@github.com/Joe-Occhipinti/unfaithfulness_steering.git"

# Install required packages
!pip install -U bitsandbytes accelerate transformers torch

"""
extract_activations.py

Step 3 of faithfulness steering workflow: Extract hidden state activations from annotated biased prompts

Extracts activations at the level of periods before closing tags in annotated prompts.
For every prompt, stores activations from every label from any layer inside separate .pt files.
"""

import time
import sys
import os
from datetime import datetime

# Import reusable modules
from src.activations import (
    extract_activations_from_annotated_prompts,
    get_activation_statistics,
    print_activation_statistics,
    build_activation_dataset,
    save_activation_dataset,
    print_dataset_summary
)
from src.config import TODAY, ACTIVATIONS_DIR, ANNOTATED_DIR, ACTIVATION_DATASETS_DIR, ActivationConfig

# =============================================================================
# ACTIVATION EXTRACTION PARAMETERS (easy to tune)
# =============================================================================

MODEL_ID = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
SOURCE_DATE = "2025-09-21"  # Update to match your annotated dataset date

# Configuration (single function call)
config = ActivationConfig.configure_extraction(SOURCE_DATE, MODEL_ID)

print(f"=== ACTIVATION EXTRACTION - {TODAY} ===")
print(f"Model: {MODEL_ID}")
print(f"Input File: {config['annotated_input_file']}")
print(f"Prompt Field: {config['prompt_field']}")
print(f"Output Directory: {config['output_dir']}")
print(f"Target Tags: {config['target_tags']}")
print(f"Layers: {len(config['layers_to_extract'])} layers (0-{max(config['layers_to_extract'])})")

# =============================================================================
# ACTIVATION EXTRACTION WORKFLOW - CELL-BY-CELL FOR COLAB
# =============================================================================

# CELL 1: Extract Activations
print("=== CELL 1: Extract Activations ===")
start_time = time.time()

# Run the main extraction function
extract_activations_from_annotated_prompts(
    jsonl_filename=config['annotated_input_file'],
    prompt_field=config['prompt_field'],
    output_dir=config['output_dir'],
    model_id=MODEL_ID,
    target_tags=config['target_tags'],
    layers_to_extract=config['layers_to_extract'],
    verbose=config['verbose']
)

# CELL 2: Compute and Display Statistics
print("\n=== CELL 2: Compute and Display Statistics ===")

# Get statistics about extracted activations
stats = get_activation_statistics(config['output_dir'])

# Print detailed report
print_activation_statistics(stats)

# CELL 3: Build Activation Dataset
print("\n=== CELL 3: Build Activation Dataset ===")

# Build aggregated dataset (layer-wise, all activations across prompts)
dataset = build_activation_dataset(
    activations_dir=config['output_dir'],
    target_tags=config['target_tags'],
    num_layers=len(config['layers_to_extract']),
    hidden_dim=4096  # DeepSeek hidden dimension
)

# Save dataset
dataset_file = f"{ACTIVATION_DATASETS_DIR}/activations_annotated_hinted_{SOURCE_DATE}.pkl"
os.makedirs(ACTIVATION_DATASETS_DIR, exist_ok=True)
save_activation_dataset(dataset, dataset_file)

# Print dataset summary
print_dataset_summary(dataset)

# CELL 4: Create Archive for Download
print("\n=== CELL 4: Create Archive for Download ===")

import shutil

# Create a zip file of the entire output directory
archive_path = f'/content/activations_{SOURCE_DATE}_archive'
shutil.make_archive(archive_path, 'zip', config['output_dir'])

archive_file = f"{archive_path}.zip"
print(f"Created archive: {archive_file}")
print(f"Archive contains {stats['total_prompts']} activation files")

# CELL 5: Summary and Completion
print("\n=== CELL 5: Summary and Completion ===")

end_time = time.time()
processing_time = end_time - start_time

print(f"\n=== ACTIVATION EXTRACTION COMPLETE ===")
print(f"✅ All README workflow requirements fulfilled:")
print(f"   ✅ Loaded annotated biased prompts from {config['annotated_input_file']}")
print(f"   ✅ Extracted activations at periods before closing tags")
print(f"   ✅ Processed {stats['total_prompts']} prompts")
print(f"   ✅ Extracted from {len(stats['layers_found'])} layers")
print(f"   ✅ Found tags: {stats['tags_found']}")
print(f"   ✅ Stored activations in individual .pt files: {config['output_dir']}")
print(f"   ✅ Maintained prompt-wise hierarchy: prompt → layer → label → activations")
print(f"   ✅ Built aggregated activation dataset: {dataset_file}")
print(f"   ✅ Dataset structure: layer → label → tensor([total_activations, hidden_dim])")

print(f"\nProcessing time: {processing_time/60:.1f} minutes")

# Save summary
summary = {
    'extraction_date': TODAY,
    'model_id': MODEL_ID,
    'input_file': config['annotated_input_file'],
    'output_dir': config['output_dir'],
    'statistics': stats,
    'processing_time_seconds': processing_time,
    'configuration': {
        'prompt_field': config['prompt_field'],
        'target_tags': config['target_tags'],
        'layers_extracted': config['layers_to_extract'],
        'verbose': config['verbose']
    }
}

import json
summary_file = f"{config['output_dir']}/extraction_summary_{TODAY}.json"
os.makedirs(os.path.dirname(summary_file), exist_ok=True)
with open(summary_file, 'w', encoding='utf-8') as f:
    json.dump(summary, f, indent=2, ensure_ascii=False)

print(f"Summary saved to {summary_file}")

print(f"\nReady for Step 4: separability analysis")
print(f"Use activation dataset: {dataset_file}")

# CELL 6: Push results to GitHub
print(f"\n--- Pushing results to GitHub ---")

# Add all activation .pt files from the output directory
!git add "{config['output_dir']}"/*.pt

# Add the extraction summary
!git add "{config['output_dir']}/extraction_summary_{TODAY}.json"

# Add the dataset file (path has spaces, needs quotes)
!git add "{dataset_file}"

# Check what will be committed
!git status

# Commit and push
!git commit -m "Add activation extraction results - {TODAY}"
!git push {repo_url} main

# CELL 7 (Optional): Download archive
from google.colab import files

files.download(archive_file)

# CELL 8 (Optional): Clean up GPU memory
import gc
import torch
torch.cuda.empty_cache()
gc.collect()
print("GPU memory cleared")