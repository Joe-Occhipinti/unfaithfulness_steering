# -*- coding: utf-8 -*-
"""tune_steering_vectors.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/
"""

# Commented out IPython magic to ensure Python compatibility.
# Setting up to work with the project GitHub Repository (importing scripts, pushing results)

# Clone the repo to import in Colab its packages from GitHub
!git clone https://github.com/Joe-Occhipinti/unfaithfulness_steering.git
import os
os.chdir('/content/unfaithfulness_steering')

# Authenticate in GitHub
!git config --global user.email "occhidipinti00@gmail.com"
!git config --global user.name "Joe-Occhipinti"

# Put your GitHub token in Colab secrets
from google.colab import userdata
GITHUB_TOKEN = userdata.get('Colab')

# Build authenticated repo url
repo_url = f"https://{GITHUB_TOKEN}@github.com/Joe-Occhipinti/unfaithfulness_steering.git"

# Install required packages
!pip install -U bitsandbytes accelerate transformers google-genai requests python-dotenv

# Set up DeepSeek API environment variables from Colab secrets
import os
os.environ['DEEPSEEK_API_KEY'] = userdata.get('DEEPSEEK_API_KEY')
os.environ['DEEPSEEK_BASE_URL'] = userdata.get('DEEPSEEK_BASE_URL') or 'https://api.deepseek.com'

# Set up Gemini API environment variables from Colab secrets
os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')

"""
tune_steering_vectors.py

Step 6 of faithfulness steering workflow: Tune steering vectors

This script:
1. Loads validation split of annotated biased prompts
2. Loads pre-computed steering vectors
3. Tests different layer-coefficient combinations
4. Evaluates faithfulness of steered outputs
5. Identifies optimal steering configuration
"""

import json
import time
import torch
import gc
import pickle
from datetime import datetime
from typing import Dict, Any, List, Tuple
from tqdm import tqdm
import random

# Import reusable modules
from src.data import load_jsonl, save_jsonl
from src.model import load_model
from src.steering import (
    apply_steering_to_model,
    generate_steered_batch,
    sweep_coefficients
)
from src.faithfulness_eval import (
    setup_gemini_client,
    annotate_batch,
    classify_faithfulness,
    compute_faithfulness_metrics,
    print_faithfulness_report
)
from src.performance_eval import (
    extract_answer_letter,
    compute_accuracy_metrics
    setup_deepseek_client,
)
from src.config import TODAY, BEHAVIOURAL_DIR, SUMMARIES_DIR, ANNOTATED_DIR, STEERING_VECTORS_DIR

# =============================================================================
# TUNABLE PARAMETERS
# =============================================================================

# Model configuration
MODEL_ID = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
BATCH_SIZE = 10  # Batch size for generation
MAX_NEW_TOKENS = 2048
MAX_INPUT_LENGTH = 1024

# Input files
ANNOTATED_BIASED_FILE = f"{ANNOTATED_DIR}/annotated_hinted_2025-09-24.jsonl"  # Update with actual date
STEERING_VECTORS_FILE = "results\steering_vectors_2025-09-24\F_vs_U\steering_vector_2025-09-24.pkl"  # Update with actual steering vector file

# Steering sweep configuration
LAYERS_TO_TEST = [15, 25]  # Middle-to-late layers typically work best
COEFFICIENTS = [0.75, -0.75, 5, -5]  # Positive and negative coefficients

# Output configuration
OUTPUT_FILE = f"{BEHAVIOURAL_DIR}/steered_val_{TODAY}.jsonl"
SUMMARY_FILE = f"{SUMMARIES_DIR}/steered_val_summary_{TODAY}.json"

print(f"=== TUNE STEERING VECTORS - {TODAY} ===")
print(f"Model: {MODEL_ID}")
print(f"Input: {ANNOTATED_BIASED_FILE}")
print(f"Steering Vectors: {STEERING_VECTORS_FILE}")
print(f"Layers to test: {LAYERS_TO_TEST}")
print(f"Coefficients: {COEFFICIENTS}")
print(f"Output: {OUTPUT_FILE}")

# =============================================================================
# STEERING TUNING WORKFLOW - CELL-BY-CELL FOR COLAB
# =============================================================================

# CELL 1: Setup and Model Loading
print("\n=== CELL 1: Setup and Model Loading ===")
start_time = time.time()

# Load model
model, tokenizer = load_model(MODEL_ID)

# Setup Gemini and DeepSeek client for (1) faithfulness evaluation, (2) format validation
gemini_client = setup_gemini_client()
deepseek_client = setup_deepseek_client()

print(f"Setup completed in {time.time() - start_time:.2f} seconds")

# CELL 2: Load Data and Steering Vectors
print("\n=== CELL 2: Load Data and Steering Vectors ===")

# Load annotated biased prompts
print(f"Loading annotated data from {ANNOTATED_BIASED_FILE}...")
annotated_data = load_jsonl(ANNOTATED_BIASED_FILE)
print(f"Loaded {len(annotated_data)} annotated examples")

# Apply same randomization and splitting as separability analysis (seed 42, 70% train / 30% val + test)
random.seed(42)
shuffled_indices = list(range(len(annotated_data)))
random.shuffle(shuffled_indices)

# Apply 70/30 split (same as separability analysis and steering vector computation)
train_size = int(0.7 * len(annotated_data))
val_indices = shuffled_indices[train_size:]  # Last 30%

print(f"Split: {train_size} train, {len(val_indices)} val (using seed 42)")

# Get validation split data
val_data = [annotated_data[i] for i in val_indices]

# Filter for unfaithful examples only (we want to test steering on unfaithful prompts)
val_unfaithful = [
    item for item in val_data
    if item.get('faithfulness_classification') == 'unfaithful'
]
print(f"Validation split: {len(val_data)} total, {len(val_unfaithful)} unfaithful examples")

# Extract hinted input prompts (use unfaithful examples for steering tuning)
hinted_prompts = [item['hinted_input_prompt'] for item in val_unfaithful]
print(f"Extracted {len(hinted_prompts)} hinted input prompts (unfaithful only)")

# Load steering vectors
print(f"\nLoading steering vectors from {STEERING_VECTORS_FILE}...")
with open(STEERING_VECTORS_FILE, 'rb') as f:
    steering_data = pickle.load(f)

steering_vectors = steering_data['steering_vectors']
available_layers = sorted(steering_vectors.keys())
print(f"Loaded steering vectors for {len(available_layers)} layers")

# Filter layers to test based on availability
layers_to_test = [l for l in LAYERS_TO_TEST if l in available_layers]
print(f"Will test {len(layers_to_test)} layers: {layers_to_test}")

# CELL 3: Coefficient and Layers Sweep (MAIN PROCESSING LOOP)
print("\n=== CELL 4: Coefficient Sweep ===")
print(f"Testing {len(layers_to_test)} layers Ã— {len(COEFFICIENTS)} coefficients = {len(layers_to_test) * len(COEFFICIENTS)} configurations")

# Run coefficient sweep
steered_results = sweep_coefficients(
    model=model,
    tokenizer=tokenizer,
    prompts=hinted_prompts,
    steering_vectors=steering_vectors,
    layers_to_test=layers_to_test,
    coefficients=COEFFICIENTS,
    batch_size=BATCH_SIZE,
    max_new_tokens=MAX_NEW_TOKENS
)

print(f"Generated steered responses for {len(steered_results)} configurations")

# CELL 4: Process Steered Results
print("\n=== CELL 4: Process Steered Results ===")

evaluation_results = {}

for (layer_idx, coeff), steered_responses in tqdm(steered_results.items(), desc="Processing steering configs"):
    print(f"\nProcessing layer {layer_idx}, coefficient {coeff:+.1f}")

    # Create steered biased prompts (prompt + steered response)
    steered_biased_prompts = [
        prompt + response
        for prompt, response in zip(hinted_prompts, steered_responses)
    ]

    # Extract answer letters from steered responses
    steered_answers = [extract_answer_letter(response) for response in steered_responses]

    # Store results for this configuration
    evaluation_results[(layer_idx, coeff)] = {
        'steered_responses': steered_responses,
        'steered_biased_prompts': steered_biased_prompts,
        'steered_answers': steered_answers
    }

print(f"Processed results for {len(evaluation_results)} steering configurations")

# CELL 5: Faithfulness Evaluation
print("\n=== CELL 5: Faithfulness Evaluation ===")

for (layer_idx, coeff), results in evaluation_results.items():
    print(f"\nEvaluating faithfulness: layer {layer_idx}, coeff {coeff:+.1f}")

    # Prepare data for faithfulness annotation (Gemini)
    batch_data = []
    for i, (biased_prompt, orig_item) in enumerate(zip(results['steered_biased_prompts'], val_unfaithful)):
        batch_data.append({
            'biased_prompt': biased_prompt,
            'ground_truth_letter': orig_item.get('ground_truth_letter'),
            'hint_letter': orig_item.get('hint_letter'),
            'model_answer': results['steered_answers'][i]
        })

    # Get faithfulness annotations from Gemini
    annotations = annotate_batch(batch_data, gemini_client)

    # Extract faithfulness labels
    faithfulness_labels = [ann.get('classification', 'error') for ann in annotations]

    # Compute improvement metrics
    original_faithful_count = 0  # All val_unfaithful were unfaithful
    steered_faithful_count = sum(1 for label in faithfulness_labels if label == 'faithful')
    improvement_rate = steered_faithful_count / len(faithfulness_labels) if faithfulness_labels else 0

    # Store evaluation results
    results.update({
        'annotations': annotations,
        'faithfulness_labels': faithfulness_labels,
        'improvement_rate': improvement_rate,
        'steered_faithful_count': steered_faithful_count,
        'total_prompts': len(faithfulness_labels)
    })

    print(f"  Improvement: {improvement_rate:.1%} ({steered_faithful_count}/{len(faithfulness_labels)} became faithful)")

print(f"Completed faithfulness evaluation for {len(evaluation_results)} configurations")

# CELL 6: Find Best Configuration
print("\n=== CELL 6: Find Best Configuration ===")

# Sort by improvement rate
sorted_configs = sorted(
    evaluation_results.items(),
    key=lambda x: x[1]['improvement_rate'],
    reverse=True
)

print("\nTop configurations by faithfulness improvement:")
for (layer_idx, coeff), results in sorted_configs[:5]:
    print(f"  Layer {layer_idx}, Coeff {coeff:+.1f}: {results['improvement_rate']:.1%}")

best_config = sorted_configs[0]
best_layer, best_coeff = best_config[0]
best_results = best_config[1]

print(f"\n*** BEST CONFIGURATION ***")
print(f"Layer: {best_layer}")
print(f"Coefficient: {best_coeff:+.2f}")
print(f"Improvement Rate: {best_results['improvement_rate']:.1%}")

# CELL 7: Generate Plots
print("\n=== CELL 7: Generate Plots ===")

import matplotlib.pyplot as plt
import numpy as np

# Separate positive and negative coefficients
pos_coeffs = [c for c in COEFFICIENTS if c > 0]
neg_coeffs = [c for c in COEFFICIENTS if c < 0]

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# Plot positive coefficients
if pos_coeffs:
    for coeff in pos_coeffs:
        improvement_rates = []
        for layer in layers_to_test:
            if (layer, coeff) in evaluation_results:
                improvement_rates.append(evaluation_results[(layer, coeff)]['improvement_rate'])
            else:
                improvement_rates.append(0)
        ax1.plot(layers_to_test, improvement_rates, marker='o', label=f'Coeff +{coeff:.1f}')

    ax1.set_xlabel('Layer')
    ax1.set_ylabel('Improvement Rate')
    ax1.set_title('Faithfulness Improvement vs Layer (Positive Coefficients)')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    ax1.set_ylim(0, 1)

# Plot negative coefficients
if neg_coeffs:
    for coeff in neg_coeffs:
        improvement_rates = []
        for layer in layers_to_test:
            if (layer, coeff) in evaluation_results:
                improvement_rates.append(evaluation_results[(layer, coeff)]['improvement_rate'])
            else:
                improvement_rates.append(0)
        ax2.plot(layers_to_test, improvement_rates, marker='o', label=f'Coeff {coeff:.1f}')

    ax2.set_xlabel('Layer')
    ax2.set_ylabel('Improvement Rate')
    ax2.set_title('Faithfulness Improvement vs Layer (Negative Coefficients)')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    ax2.set_ylim(0, 1)

plt.suptitle(f'Steering Tuning Results - Faithfulness Improvement by Layer and Coefficient', fontsize=14)
plt.tight_layout()

# Save plot
import os
os.makedirs('plots', exist_ok=True)
plot_file = f"plots/steering_tuning_faithfulness_{TODAY}.png"
plt.savefig(plot_file, dpi=150, bbox_inches='tight')
plt.show()
print(f"Plot saved to {plot_file}")

# CELL 8: Save Results
print("\n=== CELL 8: Save Results ===")

# Save detailed results
detailed_results = {}
for (layer_idx, coeff), results in evaluation_results.items():
    key = f"layer_{layer_idx}_coeff_{coeff:+.1f}"
    detailed_results[key] = {
        'layer': layer_idx,
        'coefficient': coeff,
        'improvement_rate': results['improvement_rate'],
        'steered_faithful_count': results['steered_faithful_count'],
        'total_prompts': results['total_prompts'],
        'steered_responses': results['steered_responses'],
        'steered_answers': results['steered_answers'],
        'faithfulness_labels': results['faithfulness_labels']
    }

# Save detailed results
os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
detailed_file = OUTPUT_FILE.replace('.jsonl', '_detailed.json')
with open(detailed_file, 'w', encoding='utf-8') as f:
    json.dump(detailed_results, f, indent=2, ensure_ascii=False)
print(f"Detailed results saved to {detailed_file}")

# Save summary
end_time = time.time()
summary = {
    'date': TODAY,
    'model': MODEL_ID,
    'input_file': ANNOTATED_BIASED_FILE,
    'steering_vectors_file': STEERING_VECTORS_FILE,
    'num_examples': len(val_unfaithful),
    'layers_tested': layers_to_test,
    'coefficients_tested': COEFFICIENTS,
    'best_configuration': {
        'layer': best_layer,
        'coefficient': best_coeff,
        'improvement_rate': best_results['improvement_rate'],
        'steered_faithful_count': best_results['steered_faithful_count']
    },
    'all_results': {
        f"layer_{layer}_coeff_{coeff:.1f}": {
            'improvement_rate': results['improvement_rate'],
            'steered_faithful_count': results['steered_faithful_count'],
            'total_prompts': results['total_prompts']
        }
        for (layer, coeff), results in evaluation_results.items()
    },
    'processing_time_seconds': end_time - start_time
}

with open(SUMMARY_FILE, 'w', encoding='utf-8') as f:
    json.dump(summary, f, indent=2, ensure_ascii=False)
print(f"Summary saved to {SUMMARY_FILE}")

# CELL 9: Save Best Configuration Outputs
print("\n=== CELL 9: Save Best Configuration Outputs ===")

# Save full output data for best configuration
output_data = []
for i, orig_item in enumerate(val_unfaithful):
    record = {
        'question_id': orig_item.get('question_id', i),
        'question': orig_item.get('question'),
        'hinted_input_prompt': orig_item.get('hinted_input_prompt'),
        'hint': orig_item.get('hint'),

        # Original data
        'original_biased_prompt': orig_item.get('biased_prompt', ''),
        'original_faithfulness_label': orig_item.get('faithfulness_classification', 'unfaithful'),

        # Steered results
        'steered_output': best_results['steered_responses'][i],
        'steered_biased_prompt': best_results['steered_biased_prompts'][i],
        'steered_answer_letter': best_results['steered_answers'][i],
        'steered_faithfulness_label': best_results['faithfulness_labels'][i],
        'steering_layer': best_layer,
        'steering_coefficient': best_coeff,

        # Ground truth
        'ground_truth_letter': orig_item.get('ground_truth_letter'),
        'hint_letter': orig_item.get('hint_letter'),

        # Changes
        'became_faithful': best_results['faithfulness_labels'][i] == 'faithful',

        # Metadata
        'split': 'val',
        'date': TODAY,
        'model': MODEL_ID
    }
    output_data.append(record)

save_jsonl(output_data, OUTPUT_FILE)
print(f"Saved {len(output_data)} best configuration records to {OUTPUT_FILE}")

print(f"\n=== STEERING TUNING COMPLETE ===")
print(f"Processing time: {(end_time - start_time) / 60:.2f} minutes")
print(f"\nBest steering configuration:")
print(f"  Layer: {best_layer}")
print(f"  Coefficient: {best_coeff:+.2f}")
print(f"  Faithfulness improvement: {best_results['improvement_rate']:.1%}")
print(f"\nResults saved to:")
print(f"  Data: {OUTPUT_FILE}")
print(f"  Summary: {SUMMARY_FILE}")
print(f"  Plot: {plot_file}")
print(f"\nUse these parameters in test_steering_vectors.py for final evaluation.")
