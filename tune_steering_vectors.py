# -*- coding: utf-8 -*-
"""tune_steering_vectors.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/
"""

# Commented out IPython magic to ensure Python compatibility.
# Setting up to work with the project GitHub Repository (importing scripts, pushing results)

# Clone the repo to import in Colab its packages from GitHub
!git clone https://github.com/Joe-Occhipinti/unfaithfulness_steering.git
import os
os.chdir('/content/unfaithfulness_steering')

# Authenticate in GitHub
!git config --global user.email "occhidipinti00@gmail.com"
!git config --global user.name "Joe-Occhipinti"

# Put your GitHub token in Colab secrets
from google.colab import userdata
GITHUB_TOKEN = userdata.get('Colab')

# Build authenticated repo url
repo_url = f"https://{GITHUB_TOKEN}@github.com/Joe-Occhipinti/unfaithfulness_steering.git"

# Install required packages
!pip install -U bitsandbytes accelerate transformers google-genai requests python-dotenv

# Set up DeepSeek API environment variables from Colab secrets
import os
os.environ['DEEPSEEK_API_KEY'] = userdata.get('DEEPSEEK_API_KEY')
os.environ['DEEPSEEK_BASE_URL'] = userdata.get('DEEPSEEK_BASE_URL') or 'https://api.deepseek.com'

# Set up Gemini API environment variables from Colab secrets
os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')

"""
tune_steering_vectors.py

Step 6 of faithfulness steering workflow: Tune steering vectors

This script:
1. Loads validation split of annotated biased prompts
2. Loads pre-computed steering vectors
3. Tests different layer-coefficient combinations
4. Evaluates faithfulness of steered outputs
5. Identifies optimal steering configuration
"""

import json
import time
import torch
import gc
import pickle
from datetime import datetime
from typing import Dict, Any, List, Tuple
from tqdm import tqdm

# Import reusable modules
from src.data import load_jsonl, save_jsonl
from src.model import load_model
from src.steering import (
    load_activation_dataset,
    apply_steering_to_model,
    generate_steered_batch,
    sweep_coefficients
)
from src.faithfulness_eval import (
    setup_gemini_client,
    annotate_batch,
    classify_faithfulness,
    compute_faithfulness_metrics,
    print_faithfulness_report
)
from src.performance_eval import (
    extract_answer_letter,
    compute_accuracy_metrics
    setup_deepseek_client,
)
from src.config import TODAY

# =============================================================================
# TUNABLE PARAMETERS
# =============================================================================

# Model configuration
MODEL_ID = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
BATCH_SIZE = 10  # Batch size for generation
MAX_NEW_TOKENS = 2048
MAX_INPUT_LENGTH = 1024

# Input files
ANNOTATED_BIASED_FILE = "data/annotated/annotated_hinted_2025-09-21.jsonl"  # Update with actual date
STEERING_VECTORS_FILE = "data/steering_vectors/steering_vector_F_vs_U_2025-09-21.pkl"  # Update with actual

# Steering sweep configuration
LAYERS_TO_TEST = [20, 21, 22, 23, 24, 25, 26, 27]  # Middle-to-late layers typically work best
COEFFICIENTS = [0.5, 0.75, 1.0, -0.5, -0.75, -1.0]  # Positive and negative coefficients

# Output configuration
OUTPUT_DIR = "data/behavioural"
OUTPUT_FILE = f"{OUTPUT_DIR}/steered_val_{TODAY}.jsonl"
SUMMARY_FILE = f"{OUTPUT_DIR}/steered_val_summary_{TODAY}.json"

# Validation split configuration
VAL_SPLIT = "val"  # Use validation split for tuning

print(f"=== TUNE STEERING VECTORS - {TODAY} ===")
print(f"Model: {MODEL_ID}")
print(f"Input: {ANNOTATED_BIASED_FILE}")
print(f"Steering Vectors: {STEERING_VECTORS_FILE}")
print(f"Layers to test: {LAYERS_TO_TEST}")
print(f"Coefficients: {COEFFICIENTS}")
print(f"Output: {OUTPUT_FILE}")

# =============================================================================
# STEERING TUNING WORKFLOW - CELL-BY-CELL FOR COLAB
# =============================================================================

# CELL 1: Setup and Model Loading
print("\n=== CELL 1: Setup and Model Loading ===")
start_time = time.time()

# Load model
model, tokenizer = load_model(MODEL_ID)

# Setup Gemini client for faithfulness evaluation
gemini_client = setup_gemini_client()
deepseek_client = setup_deepseek_client()

print(f"Setup completed in {time.time() - start_time:.2f} seconds")

# CELL 2: Load Data and Steering Vectors
print("\n=== CELL 2: Load Data and Steering Vectors ===")

# Load annotated biased prompts
print(f"Loading annotated data from {ANNOTATED_BIASED_FILE}...")
annotated_data = load_jsonl(ANNOTATED_BIASED_FILE)
print(f"Loaded {len(annotated_data)} annotated examples")

# Apply same randomization and splitting as separability analysis (seed 42, 70% train / 30% val)
import random
random.seed(42)
shuffled_indices = list(range(len(annotated_data)))
random.shuffle(shuffled_indices)

# Apply 70/30 split (same as separability analysis and steering vector computation)
train_size = int(0.7 * len(annotated_data))
val_indices = shuffled_indices[train_size:]  # Last 30%

print(f"Split: {train_size} train, {len(val_indices)} val (using seed 42)")

# Get validation split data
val_data = [annotated_data[i] for i in val_indices]

# Filter for unfaithful examples only (we want to test steering on unfaithful prompts)
val_unfaithful = [
    item for item in val_data
    if item.get('faithfulness_classification') == 'unfaithful'
]
print(f"Validation split: {len(val_data)} total, {len(val_unfaithful)} unfaithful examples")

# Extract hinted input prompts (use unfaithful examples for steering tuning)
hinted_prompts = [item['hinted_input_prompt'] for item in val_unfaithful]
print(f"Extracted {len(hinted_prompts)} hinted input prompts (unfaithful only)")

# Load steering vectors
print(f"\nLoading steering vectors from {STEERING_VECTORS_FILE}...")
with open(STEERING_VECTORS_FILE, 'rb') as f:
    steering_data = pickle.load(f)

steering_vectors = steering_data['steering_vectors']
available_layers = sorted(steering_vectors.keys())
print(f"Loaded steering vectors for {len(available_layers)} layers")

# Filter layers to test based on availability
layers_to_test = [l for l in LAYERS_TO_TEST if l in available_layers]
print(f"Will test {len(layers_to_test)} layers: {layers_to_test}")

# CELL 3: Coefficient and Layers Sweep (MAIN PROCESSING LOOP)
print("\n=== CELL 4: Coefficient Sweep ===")
print(f"Testing {len(layers_to_test)} layers Ã— {len(COEFFICIENTS)} coefficients = {len(layers_to_test) * len(COEFFICIENTS)} configurations")

# Run coefficient sweep
steered_results = sweep_coefficients(
    model=model,
    tokenizer=tokenizer,
    prompts=hinted_prompts,
    steering_vectors=steering_vectors,
    layers_to_test=layers_to_test,
    coefficients=COEFFICIENTS,
    batch_size=BATCH_SIZE,
    max_new_tokens=MAX_NEW_TOKENS
)

print(f"Generated steered responses for {len(steered_results)} configurations")

# CELL 4: Evaluate Faithfulness of Steered Outputs
print("\n=== CELL 5: Evaluate Faithfulness ===")

evaluation_results = {}

# Evaluate each steering configuration
for (layer_idx, coeff), steered_responses in tqdm(steered_results.items(), desc="Evaluating configurations"):
    print(f"\nEvaluating layer {layer_idx}, coefficient {coeff:+.1f}")

    # Create steered biased prompts
    steered_biased_prompts = [
        prompt + response
        for prompt, response in zip(hinted_prompts, steered_responses)
    ]

    # Prepare input data for annotate_batch
    batch_data = []
    for i, (biased_prompt, orig_item) in enumerate(zip(steered_biased_prompts, val_unfaithful)):
        batch_data.append({
            'biased_prompt': biased_prompt,
            'correct_answer': orig_item.get('ground_truth_letter'),
            'hint_letter': orig_item.get('hint_letter')
            'model_answer': extract_answer_letter(biased_prompt).
        })

        correct_answer = result.get('ground_truth_letter', result.get('correct_answer'))
        hint_letter = result.get('hint_letter', result.get('hinted_answer'))
        model_answer = result.get('hinted_answer_letter', result.get('answer_letter'))

    # Annotate steered prompts (this returns classification results)
    annotations = annotate_batch(
        results=batch_data,
        client_config=gemini_client
    )

    # Validate steered responses with DeepSeek (same process as hinted_eval.py)
    validations = validate_responses_deepseek(steered_responses, deepseek_client)

    # Process validation results
    steered_validation_data = []
    for validation in validations:
        format_followed, response_complete, answer_letter = extract_validation_data(validation)
        steered_validation_data.append({
            'format_followed': format_followed,
            'response_complete': response_complete,
            'answer_letter': answer_letter
        })

    # Extract classifications and answers
    steered_faithfulness = [ann.get('classification', 'error') for ann in annotations]
    steered_answers = [val_data['answer_letter'] for val_data in steered_validation_data]

    # Compute metrics
    metrics = compute_faithfulness_metrics(steered_faithfulness)

    # Compare faithfulness distributions: steered vs original hinted
    # Original hinted prompts were all unfaithful (that's why we selected them)
    original_faithful_count = 0  # All selected prompts were unfaithful
    steered_faithful_count = sum(1 for label in steered_faithfulness if label == 'faithful')

    # Compute faithfulness improvement rate
    faithfulness_improvement_rate = steered_faithful_count / len(steered_faithfulness) if steered_faithfulness else 0

    # Compute completeness metrics for steered responses
    steered_completeness_metrics = compute_completeness_metrics(steered_validation_data)

    # Store results
    evaluation_results[(layer_idx, coeff)] = {
        'faithfulness_improvement_rate': faithfulness_improvement_rate,
        'original_faithful_count': original_faithful_count,
        'steered_faithful_count': steered_faithful_count,
        'total_prompts': len(steered_faithfulness),
        'steered_responses': steered_responses,
        'steered_biased_prompts': steered_biased_prompts,
        'steered_annotations': steered_annotations,
        'steered_faithfulness_labels': steered_faithfulness,
        'steered_answers': steered_answers,
        'steered_validation_data': steered_validation_data,
        'completeness_metrics': steered_completeness_metrics
    }

    print(f"  Faithfulness improvement: {faithfulness_improvement_rate:.1%} ({steered_faithful_count}/{len(steered_faithfulness)} became faithful)")
    print(f"  Completeness rate: {steered_completeness_metrics['completeness_rate']:.1%} ({steered_completeness_metrics['complete_responses']}/{steered_completeness_metrics['total_responses']})")

# CELL 6: Find Best Configuration and Generate Plots
print("\n=== CELL 6: Find Best Configuration and Generate Plots ===")

# Sort by faithfulness improvement rate
sorted_configs = sorted(
    evaluation_results.items(),
    key=lambda x: x[1]['faithfulness_improvement_rate'],
    reverse=True
)

print("\nTop 5 configurations by faithfulness improvement:")
for (layer_idx, coeff), metrics in sorted_configs[:5]:
    print(f"  Layer {layer_idx}, Coeff {coeff:+.1f}: "
          f"Improvement={metrics['faithfulness_improvement_rate']:.1%}")

best_config = sorted_configs[0]
best_layer, best_coeff = best_config[0]
best_metrics = best_config[1]

print(f"\n*** BEST CONFIGURATION ***")
print(f"Layer: {best_layer}")
print(f"Coefficient: {best_coeff:+.2f}")
print(f"Faithfulness Improvement: {best_metrics['faithfulness_improvement_rate']:.1%}")

# Generate plots
import matplotlib.pyplot as plt
import numpy as np

# Separate positive and negative coefficients
pos_coeffs = [c for c in COEFFICIENTS if c > 0]
neg_coeffs = [c for c in COEFFICIENTS if c < 0]

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

# Plot positive coefficients
for coeff in pos_coeffs:
    faithfulness_rates = []
    for layer in layers_to_test:
        if (layer, coeff) in evaluation_results:
            faithfulness_rates.append(evaluation_results[(layer, coeff)]['faithfulness_rate'])
        else:
            faithfulness_rates.append(0)
    ax1.plot(layers_to_test, faithfulness_rates, marker='o', label=f'Coeff +{coeff:.1f}')

ax1.set_xlabel('Layer')
ax1.set_ylabel('Faithfulness Rate')
ax1.set_title('Faithfulness Rate vs Layer (Positive Coefficients)')
ax1.legend()
ax1.grid(True, alpha=0.3)
ax1.set_ylim(0, 1)

# Plot negative coefficients
for coeff in neg_coeffs:
    faithfulness_rates = []
    for layer in layers_to_test:
        if (layer, coeff) in evaluation_results:
            faithfulness_rates.append(evaluation_results[(layer, coeff)]['faithfulness_rate'])
        else:
            faithfulness_rates.append(0)
    ax2.plot(layers_to_test, faithfulness_rates, marker='o', label=f'Coeff {coeff:.1f}')

ax2.set_xlabel('Layer')
ax2.set_ylabel('Faithfulness Rate')
ax2.set_title('Faithfulness Rate vs Layer (Negative Coefficients)')
ax2.legend()
ax2.grid(True, alpha=0.3)
ax2.set_ylim(0, 1)

plt.suptitle(f'Steering Tuning Results - Faithfulness Rate by Layer and Coefficient', fontsize=14)
plt.tight_layout()

# Save plot
plot_file = f"plots/steering_tuning_faithfulness_{TODAY}.png"
os.makedirs('plots', exist_ok=True)
plt.savefig(plot_file, dpi=150, bbox_inches='tight')
plt.show()
print(f"Saved plot to {plot_file}")

# CELL 7: Save Results
print("\n=== CELL 7: Save Results ===")

# Save detailed results organized by layer*coefficient
detailed_results = {}
for (layer_idx, coeff), results in evaluation_results.items():
    key = f"layer_{layer_idx}_coeff_{coeff:+.1f}"
    detailed_results[key] = {
        'layer': layer_idx,
        'coefficient': coeff,
        'faithfulness_rate': results['faithfulness_rate'],
        'became_faithful_count': results['became_faithful_count'],
        'total_prompts': results['total_prompts'],
        'steered_responses': results['steered_responses'],
        'steered_answers': results['steered_answers'],
        'steered_faithfulness_labels': results['steered_faithfulness_labels'],
        'completeness_metrics': results['completeness_metrics']
    }

# Save detailed results
detailed_file = f"{OUTPUT_DIR}/steered_detailed_{TODAY}.json"
with open(detailed_file, 'w', encoding='utf-8') as f:
    json.dump(detailed_results, f, indent=2, ensure_ascii=False)
print(f"Saved detailed results to {detailed_file}")

# Save summary
summary = {
    'date': TODAY,
    'model': MODEL_ID,
    'input_file': ANNOTATED_BIASED_FILE,
    'steering_vectors_file': STEERING_VECTORS_FILE,
    'num_examples': len(val_unfaithful),
    'layers_tested': layers_to_test,
    'coefficients_tested': COEFFICIENTS,
    'best_configuration': {
        'layer': best_layer,
        'coefficient': best_coeff,
        'faithfulness_rate': best_metrics['faithfulness_rate'],
        'became_faithful_count': best_metrics['became_faithful_count']
    },
    'all_results': {
        f"layer_{layer}_coeff_{coeff:.1f}": {
            'faithfulness_rate': results['faithfulness_rate'],
            'became_faithful_count': results['became_faithful_count'],
            'total_prompts': results['total_prompts']
        }
        for (layer, coeff), results in evaluation_results.items()
    }
}

with open(SUMMARY_FILE, 'w', encoding='utf-8') as f:
    json.dump(summary, f, indent=2, ensure_ascii=False)
print(f"Saved summary to {SUMMARY_FILE}")

# CELL 8: Save Best Configuration Outputs
print("\n=== CELL 8: Save Best Configuration Outputs ===")

# Save full output data for best configuration
output_data = []
best_results = evaluation_results[(best_layer, best_coeff)]

for i, orig_item in enumerate(val_unfaithful):
    record = {
        'question_id': orig_item.get('question_id', i),
        'question': orig_item.get('question'),
        'hinted_input_prompt': orig_item.get('hinted_input_prompt'),
        'hint': orig_item.get('hint'),

        # Original data
        'original_biased_prompt': orig_item.get('biased_prompt', ''),
        'original_faithfulness_label': orig_item.get('faithfulness_classification', 'unfaithful'),

        # Steered results
        'steered_output': best_results['steered_responses'][i],
        'steered_biased_prompt': best_results['steered_biased_prompts'][i],
        'steered_answer_letter': best_results['steered_answers'][i],
        'steered_faithfulness_label': best_results['steered_faithfulness_labels'][i],
        'steering_layer': best_layer,
        'steering_coefficient': best_coeff,

        # Ground truth
        'ground_truth_letter': orig_item.get('ground_truth_letter'),
        'hinted_letter': orig_item.get('hinted_letter'),

        # Validation metadata (same as hinted_eval.py)
        'format_followed': best_results['steered_validation_data'][i]['format_followed'],
        'response_complete': best_results['steered_validation_data'][i]['response_complete'],

        # Changes
        'became_faithful': best_results['steered_faithfulness_labels'][i] == 'faithful',

        # Metadata
        'split': 'val',
        'date': TODAY,
        'model': MODEL_ID
    }
    output_data.append(record)

save_jsonl(output_data, OUTPUT_FILE)
print(f"Saved {len(output_data)} best configuration records to {OUTPUT_FILE}")



# CELL 9: Push Results to GitHub
print("\n=== CELL 9: Push Results to GitHub ===")

# Add files
!git add {OUTPUT_FILE} {SUMMARY_FILE} {detailed_file} {plot_file}

# Commit
commit_message = f"Steering tuning results - {TODAY}"
!git commit -m "{commit_message}"

# Push to GitHub
!git push {repo_url} main

print(f"Results pushed to GitHub successfully!")

# CELL 10: Final Summary
print("\n=== FINAL SUMMARY ===")
print(f"Tuning completed in {(time.time() - start_time) / 60:.2f} minutes")
print(f"\nBest steering configuration:")
print(f"  Layer: {best_layer}")
print(f"  Coefficient: {best_coeff:+.2f}")
print(f"  Faithfulness improvement: {best_metrics['faithfulness_rate'] - baseline_metrics['faithfulness_rate']:.1%}")
print(f"\nResults saved to:")
print(f"  Data: {OUTPUT_FILE}")
print(f"  Summary: {SUMMARY_FILE}")
print(f"\nUse these parameters in test_steering_vectors.py for final evaluation.")