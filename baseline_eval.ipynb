{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNeoXrF5hjW3hM90n0P6ize",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joe-Occhipinti/unfaithfulness_steering/blob/main/baseline_eval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up to work with the project GitHub Repository (importing scripts, pushing results)\n",
        "\n",
        "# clone the repo to import in Colab its packages from GitHub\n",
        "!git clone https://github.com/Joe-Occhipinti/unfaithfulness_steering.git\n",
        "%cd /content/unfaithfulness_steering\n",
        "\n",
        "# authenticate in GitHub\n",
        "!git config --global user.email \"occhidipinti00@gmail.com\"\n",
        "!git config --global user.name \"Joe-Occhipinti\"\n",
        "\n",
        "# put your GitHub token in Colab secrets\n",
        "from google.colab import userdata\n",
        "GITHUB_TOKEN = userdata.get('Colab')\n",
        "\n",
        "# build authenticated repo url\n",
        "repo_url = f\"https://{GITHUB_TOKEN}@github.com/Joe-Occhipinti/unfaithfulness_steering.git\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQCuNTvNZDhI",
        "outputId": "09d5335b-4db9-4632-ec78-ddef2b89d945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'unfaithfulness_steering'...\n",
            "remote: Enumerating objects: 588, done.\u001b[K\n",
            "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 588 (delta 19), reused 32 (delta 11), pack-reused 545 (from 1)\u001b[K\n",
            "Receiving objects: 100% (588/588), 290.79 MiB | 35.33 MiB/s, done.\n",
            "Resolving deltas: 100% (304/304), done.\n",
            "Updating files: 100% (464/464), done.\n",
            "/content/unfaithfulness_steering\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes accelerate transformers google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0Vg21MHQk_D2",
        "outputId": "cd6c6ea8-250e-41e7-b72e-505ebe65fc8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.10.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.56.2-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: google-genai in /usr/local/lib/python3.12/dist-packages (1.34.0)\n",
            "Collecting google-genai\n",
            "  Downloading google_genai-1.38.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.34.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.10.0)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.38.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from google-genai) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (2.11.7)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai) (8.5.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (15.0.1)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /usr/local/lib/python3.12/dist-packages (from google-genai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.9)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai) (0.6.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.56.2-py3-none-any.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_genai-1.38.0-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.6/245.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google-genai, transformers, bitsandbytes\n",
            "  Attempting uninstall: google-genai\n",
            "    Found existing installation: google-genai 1.34.0\n",
            "    Uninstalling google-genai-1.34.0:\n",
            "      Successfully uninstalled google-genai-1.34.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.56.1\n",
            "    Uninstalling transformers-4.56.1:\n",
            "      Successfully uninstalled transformers-4.56.1\n",
            "Successfully installed bitsandbytes-0.47.0 google-genai-1.38.0 transformers-4.56.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "85a340ec56854c99b5c3d1942a60272a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "baseline_eval.py\n",
        "\n",
        "Step 1 of faithfulness steering workflow: Baseline evaluation on MMLU\n",
        "\n",
        "Uses reusable modules from src/ for common functionality.\n",
        "Only contains baseline-specific logic inline.\n",
        "\"\"\"\n",
        "\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "# Import reusable modules\n",
        "from src.data import load_mmlu_simple, save_jsonl, convert_answer_to_letter\n",
        "from src.model import load_model, batch_generate\n",
        "from src.performance_eval import setup_deepseek_client, validate_responses_deepseek, compute_accuracy_metrics, print_accuracy_report\n",
        "from src.config import BaselineConfig, TODAY\n",
        "from src.prompts import create_baseline_prompts"
      ],
      "metadata": {
        "id": "ORc28MAKnCZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# BASELINE-SPECIFIC MODEL & GENERATION PARAMETERS (easy to tune)\n",
        "# =============================================================================\n",
        "\n",
        "MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
        "BATCH_SIZE = 10\n",
        "MAX_NEW_TOKENS = 2048\n",
        "MAX_INPUT_LENGTH = 1024\n",
        "\n",
        "print(f\"=== BASELINE EVALUATION - {TODAY} ===\")\n",
        "print(f\"Model: {MODEL_ID}\")\n",
        "print(f\"MMLU Subjects: {BaselineConfig.SUBJECTS}\")\n",
        "print(f\"Output: {BaselineConfig.OUTPUT_FILE}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE}, Max New Tokens: {MAX_NEW_TOKENS}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0RzbHYSnIfz",
        "outputId": "9bc7a30e-ec81-4af5-8c85-60ff5716f2ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== BASELINE EVALUATION - 2025-09-21 ===\n",
            "Model: deepseek-ai/DeepSeek-R1-Distill-Llama-8B\n",
            "MMLU Subjects: ['high_school_psychology']\n",
            "Output: data/behavioural/baseline_2025-09-21.jsonl\n",
            "Batch Size: 10, Max New Tokens: 2048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# BASELINE EVALUATION WORKFLOW - CELL-BY-CELL FOR COLAB\n",
        "# =============================================================================\n",
        "\n",
        "# CELL 1: Setup and Model Loading\n",
        "print(\"=== CELL 1: Setup and Model Loading ===\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Load model (reusable)\n",
        "model, tokenizer = load_model(MODEL_ID)"
      ],
      "metadata": {
        "id": "enDjV5IUoCxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 2: Data Loading and Prompt Creation\n",
        "print(\"\\n=== CELL 2: Data Loading and Prompt Creation ===\")\n",
        "\n",
        "# Load MMLU data (reusable)\n",
        "mmlu_data = load_mmlu_simple(BaselineConfig.SUBJECTS)[0:5]\n",
        "\n",
        "# Create baseline prompts (from prompts module)\n",
        "baseline_prompts = create_baseline_prompts(mmlu_data)\n",
        "\n",
        "print(f\"\\n--- Ready to process {len(baseline_prompts)} prompts ---\")"
      ],
      "metadata": {
        "id": "ghhjjOePuDdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 3: Text Generation (can run separately)\n",
        "print(\"\\n=== CELL 3: Text Generation ===\")\n",
        "\n",
        "# Generate responses (reusable)\n",
        "all_answers = batch_generate(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    prompts=baseline_prompts,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    max_new_tokens=MAX_NEW_TOKENS,\n",
        "    max_input_length=MAX_INPUT_LENGTH\n",
        ")"
      ],
      "metadata": {
        "id": "KZyki2hzuGJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 4: Validation with DeepSeek (can run separately)\n",
        "print(\"\\n=== CELL 4: Validation with DeepSeek ===\")\n",
        "\n",
        "# Setup DeepSeek validation (reusable)\n",
        "deepseek_client = setup_deepseek_client()\n",
        "\n",
        "# Validate responses with DeepSeek (reusable)\n",
        "validations = validate_responses_deepseek(all_answers, deepseek_client)"
      ],
      "metadata": {
        "id": "yc5VTvC-uJO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 5: Processing Results\n",
        "print(\"\\n=== CELL 5: Processing and Saving Results ===\")\n",
        "\n",
        "# Process results (baseline-specific structure)\n",
        "print(f\"\\n--- Processing baseline results ---\")\n",
        "results = []\n",
        "\n",
        "for i, (mmlu_item, baseline_prompt, generated_answer, validation) in enumerate(\n",
        "    zip(mmlu_data, baseline_prompts, all_answers, validations)\n",
        "):\n",
        "    # Extract validation data from Gemini\n",
        "    format_followed = validation.get('format_followed', False)\n",
        "    response_complete = validation.get('response_complete', True)\n",
        "    answer_letter = validation.get('final_answer', None)  # This is the extracted letter\n",
        "\n",
        "    # Get ground truth letter (reusable)\n",
        "    ground_truth_letter = convert_answer_to_letter(mmlu_item['answer'])\n",
        "\n",
        "    # Label correctness\n",
        "    is_correct = (answer_letter == ground_truth_letter) if answer_letter is not None else False\n",
        "    accuracy_label = 'correct' if is_correct else 'wrong'\n",
        "\n",
        "    # Create baseline result record (essential data only)\n",
        "    result = {\n",
        "        # Original MMLU data\n",
        "        'question': mmlu_item['question'],\n",
        "        'subject': mmlu_item['subject'],\n",
        "        'choices': mmlu_item['choices'],\n",
        "        'answer': mmlu_item['answer'],  # Original index\n",
        "\n",
        "        # Baseline prompts and generation (README requirement)\n",
        "        'baseline_input_prompt': baseline_prompt,\n",
        "        'baseline_generated_text': generated_answer,\n",
        "        'baseline_prompt': baseline_prompt + generated_answer,\n",
        "\n",
        "        # Extracted answers (README requirement - via Gemini)\n",
        "        'answer_letter': answer_letter,  # Extracted by Gemini\n",
        "        'ground_truth_letter': ground_truth_letter,  # Converted from index\n",
        "\n",
        "        # Accuracy labels (README requirement)\n",
        "        'accuracy_label': accuracy_label\n",
        "    }\n",
        "\n",
        "    results.append(result)\n",
        "\n",
        "# Compute accuracy metrics (reusable)\n",
        "metrics = compute_accuracy_metrics(results)\n",
        "\n",
        "# Print report (reusable)\n",
        "print_accuracy_report(metrics)"
      ],
      "metadata": {
        "id": "1pXvzPvQn-8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 6: Saving Results\n",
        "\n",
        "# Save results (baseline-specific paths and summary)\n",
        "print(f\"\\n--- Saving baseline results ---\")\n",
        "\n",
        "# Save detailed results\n",
        "save_jsonl(results, BaselineConfig.OUTPUT_FILE)\n",
        "print(f\"Saved {len(results)} results to {BaselineConfig.OUTPUT_FILE}\")\n",
        "\n",
        "# Save summary metrics\n",
        "end_time = time.time()\n",
        "summary = {\n",
        "    'evaluation_date': TODAY,\n",
        "    'model_id': MODEL_ID,\n",
        "    'mmlu_subjects': BaselineConfig.SUBJECTS,\n",
        "    'metrics': metrics,\n",
        "    'processing_time_seconds': end_time - start_time,\n",
        "    'validation_method': 'gemini-2.5-flash-lite',\n",
        "    'configuration': {\n",
        "        'batch_size': BATCH_SIZE,\n",
        "        'max_new_tokens': MAX_NEW_TOKENS,\n",
        "        'max_input_length': MAX_INPUT_LENGTH\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(BaselineConfig.SUMMARY_FILE, 'w', encoding='utf-8') as f:\n",
        "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Summary saved to {BaselineConfig.SUMMARY_FILE}\")\n",
        "\n",
        "print(f\"\\n=== BASELINE EVALUATION COMPLETE ===\")\n",
        "print(f\"✅ All README workflow requirements fulfilled:\")\n",
        "print(f\"   ✅ Loaded MMLU subjects\")\n",
        "print(f\"   ✅ Created baseline input prompts\")\n",
        "print(f\"   ✅ Generated text with model\")\n",
        "print(f\"   ✅ Validated format with Gemini\")\n",
        "print(f\"   ✅ Extracted answer letters\")\n",
        "print(f\"   ✅ Computed accuracy and labeled correct/wrong\")\n",
        "print(f\"   ✅ Stored all required output data fields\")\n",
        "print(f\"\\nReady for Step 2: hinted_eval.py\")\n",
        "print(f\"Use baseline data: {BaselineConfig.OUTPUT_FILE}\")"
      ],
      "metadata": {
        "id": "bWzZQ-z2_FC9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b58debb-a53c-4c8d-9727-d8e265dfcf82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Saving baseline results ---\n",
            "Saved 610 results to data/behavioural/baseline_2025-09-21.jsonl\n",
            "Summary saved to data/behavioural/baseline_summary_2025-09-21.json\n",
            "\n",
            "=== BASELINE EVALUATION COMPLETE ===\n",
            "✅ All README workflow requirements fulfilled:\n",
            "   ✅ Loaded MMLU subjects\n",
            "   ✅ Created baseline input prompts\n",
            "   ✅ Generated text with model\n",
            "   ✅ Validated format with Gemini\n",
            "   ✅ Extracted answer letters\n",
            "   ✅ Computed accuracy and labeled correct/wrong\n",
            "   ✅ Stored all required output data fields\n",
            "\n",
            "Ready for Step 2: hinted_eval.py\n",
            "Use baseline data: data/behavioural/baseline_2025-09-21.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pushing results to github\n",
        "!git add data/generated.csv\n",
        "!git commit -m \"Add generated data\"\n",
        "!git push $repo_url main"
      ],
      "metadata": {
        "id": "e5ad2qU84Inz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 7 (Optional): Clean up GPU memory\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(\"GPU memory cleared\")"
      ],
      "metadata": {
        "id": "yqYokau9_AnC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}