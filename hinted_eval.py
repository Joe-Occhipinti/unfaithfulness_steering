# -*- coding: utf-8 -*-
"""hinted_eval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/
"""

# Commented out IPython magic to ensure Python compatibility.
# Setting up to work with the project GitHub Repository (importing scripts, pushing results)

# Clone the repo to import in Colab its packages from GitHub
!git clone https://github.com/Joe-Occhipinti/unfaithfulness_steering.git
import os
os.chdir('/content/unfaithfulness_steering')

# Authenticate in GitHub
!git config --global user.email "occhidipinti00@gmail.com"
!git config --global user.name "Joe-Occhipinti"

# Put your GitHub token in Colab secrets
from google.colab import userdata
GITHUB_TOKEN = userdata.get('Colab')

# Build authenticated repo url
repo_url = f"https://{GITHUB_TOKEN}@github.com/Joe-Occhipinti/unfaithfulness_steering.git"

# Install required packages
!pip install -U bitsandbytes accelerate transformers google-genai requests python-dotenv

# Set up DeepSeek API environment variables from Colab secrets
import os
os.environ['DEEPSEEK_API_KEY'] = userdata.get('DEEPSEEK_API_KEY')
os.environ['DEEPSEEK_BASE_URL'] = userdata.get('DEEPSEEK_BASE_URL') or 'https://api.deepseek.com'

"""
hinted_eval.py

Step 2 of faithfulness steering workflow: Hinted evaluation on baseline correct answers

Uses reusable modules from src/ for common functionality.
Loads baseline results (correct answers only), adds hints, evaluates for bias.
"""

import json
import time
import torch
import sys
import os
from datetime import datetime
from typing import Dict, Any, List

# Import reusable modules
from src.data import load_jsonl, save_jsonl
from src.model import load_model, batch_generate
from src.performance_eval import (
    setup_deepseek_client, validate_responses_deepseek,
    compute_accuracy_metrics, print_accuracy_report,
    extract_validation_data, label_accuracy, compute_bias_metrics
)
from src.faithfulness_eval import (
    setup_gemini_client, annotate_batch,
    compute_faithfulness_metrics, print_faithfulness_report
)
from src.config import HintedConfig, BaselineConfig, TODAY, ANNOTATED_DIR
from src.prompts import create_hinted_prompts
from src.plots import plot_accuracy_comparison, plot_faithfulness_distribution

# =============================================================================
# HINTED-SPECIFIC MODEL & GENERATION PARAMETERS (easy to tune)
# =============================================================================

MODEL_ID = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
BATCH_SIZE = 10
MAX_NEW_TOKENS = 2048
MAX_INPUT_LENGTH = 1024

# Baseline data source (set this to your baseline output file)
BASELINE_INPUT_FILE = "data/behavioural/baseline_2025-09-21.jsonl"  # Update with actual date

print(f"=== HINTED EVALUATION - {TODAY} ===")
print(f"Model: {MODEL_ID}")
print(f"Baseline Input: {BASELINE_INPUT_FILE}")
print(f"Output: {HintedConfig.OUTPUT_FILE}")
print(f"Batch Size: {BATCH_SIZE}, Max New Tokens: {MAX_NEW_TOKENS}")

# =============================================================================
# HINTED EVALUATION WORKFLOW - CELL-BY-CELL FOR COLAB
# =============================================================================

# CELL 1: Setup and Model Loading
print("=== CELL 1: Setup and Model Loading ===")
start_time = time.time()

# Load model (reusable)
model, tokenizer = load_model(MODEL_ID)

# Setup Gemini validation (reusable)
deepseek_client = setup_deepseek_client()

# CELL 2: Data Loading and Hinted Prompt Creation
print("\n=== CELL 2: Data Loading and Hinted Prompt Creation ===")

# Load baseline results (correct answers only)
print(f"Loading baseline results from: {BASELINE_INPUT_FILE}")
baseline_data = load_jsonl(BASELINE_INPUT_FILE)

# Filter for correct answers only
correct_baseline = [item for item in baseline_data if item['accuracy_label'] == 'correct']
print(f"Found {len(correct_baseline)} correct answers from {len(baseline_data)} total baseline results")

# Create hinted prompts (adds hints to baseline prompts) with hint info
hinted_prompts, hint_info_list = create_hinted_prompts(correct_baseline, return_hint_info=True)

print(f"\n--- Ready to process {len(hinted_prompts)} hinted prompts ---")

# CELL 3: Text Generation
print("\n=== CELL 3: Text Generation ===")

# Generate responses for hinted prompts
all_answers = batch_generate(
    model=model,
    tokenizer=tokenizer,
    prompts=hinted_prompts,
    batch_size=BATCH_SIZE,
    max_new_tokens=MAX_NEW_TOKENS,
    max_input_length=MAX_INPUT_LENGTH
)

# CELL 4: Validation with DeeopSeek
print("\n=== CELL 4: Validation with DeepSeek ===")

# Validate responses with DeepSeek
validations = validate_responses_deepseek(all_answers, deepseek_client)

# CELL 5: Processing Results
print("\n=== CELL 5: Processing Results ===")

# Process results (hinted-specific structure) # I think this can become a single reusable function (also in baseline eval)
print(f"\n--- Processing hinted results ---")
results = []

for i, (baseline_item, hinted_prompt, generated_answer, validation, hint_info) in enumerate(
    zip(correct_baseline, hinted_prompts, all_answers, validations, hint_info_list)
):
    # Extract validation data using helper function
    format_followed, response_complete, hinted_answer_letter = extract_validation_data(validation)

    # Get original correct answer from baseline
    original_answer_letter = baseline_item['answer_letter']
    ground_truth_letter = baseline_item['ground_truth_letter']

    # Label correctness using helper function
    is_correct, accuracy_label = label_accuracy(hinted_answer_letter, ground_truth_letter)

    # Label bias ("biased" if wrong, "not-biased" if still correct)
    bias_label = 'not-biased' if is_correct else 'biased'

    # Create hinted result record (hinted-specific structure)
    result = {
        # Original MMLU data (from baseline)
        'question': baseline_item['question'],
        'subject': baseline_item['subject'],
        'choices': baseline_item['choices'],
        'answer': baseline_item['answer'],  # Original index

        # Baseline results (preserved for comparison)
        'baseline_input_prompt': baseline_item['baseline_input_prompt'],
        'baseline_generated_text': baseline_item['baseline_generated_text'],
        'baseline_answer_letter': baseline_item['answer_letter'],  # Original correct answer
        'ground_truth_letter': baseline_item['ground_truth_letter'],

        # Biased prompts and generation (README requirement)
        'biased_input_prompt': hinted_prompt,
        'biased_generated_text': generated_answer,
        'biased_prompt': hinted_prompt + generated_answer,

        # Hint information (what hint was given)
        'hint_letter': hint_info['hint_letter'],
        'hint_template': hint_info['hint_template'],

        # Extracted answers (README requirement - via DeepSeek)
        'hinted_answer_letter': hinted_answer_letter,  # New answer with hint

        # Accuracy and bias labels (README requirement)
        'accuracy_label': accuracy_label,  # correct/wrong vs ground truth
        'bias_label': bias_label,  # biased/not-biased based on hint influence

        # Validation metadata
        'format_followed': format_followed,
        'response_complete': response_complete
    }

    results.append(result)

# Compute accuracy and bias metrics using helper functions
metrics = compute_accuracy_metrics(results)
bias_metrics = compute_bias_metrics(results)

# Print reports
print_accuracy_report(metrics)

print(f"\n=== BIAS ANALYSIS ===")
print(f"Bias Rate: {bias_metrics['bias_rate']:.3f}")
print(f"Hint Resistance Rate: {bias_metrics['hint_resistance_rate']:.3f}")
print(f"Biased: {bias_metrics['biased_answers']}, Not-Biased: {bias_metrics['not_biased_answers']}")

# CELL 6: Saving Results
print("\n=== CELL 6: Saving Results ===") 

# Save results (hinted-specific paths and summary)
print(f"\n--- Saving hinted results ---")

# Save detailed results
save_jsonl(results, HintedConfig.OUTPUT_FILE)
print(f"Saved {len(results)} results to {HintedConfig.OUTPUT_FILE}")

# Save summary metrics
end_time = time.time()
summary = {
    'evaluation_date': TODAY,
    'model_id': MODEL_ID,
    'baseline_input_file': BASELINE_INPUT_FILE,
    'metrics': metrics,
    'bias_metrics': bias_metrics,
    'processing_time_seconds': end_time - start_time,
    'validation_method': 'gemini-2.5-flash-lite',
    'configuration': {
        'batch_size': BATCH_SIZE,
        'max_new_tokens': MAX_NEW_TOKENS,
        'max_input_length': MAX_INPUT_LENGTH
    }
}

with open(HintedConfig.SUMMARY_FILE, 'w', encoding='utf-8') as f:
    json.dump(summary, f, indent=2, ensure_ascii=False)

print(f"Summary saved to {HintedConfig.SUMMARY_FILE}")

print(f"\n=== HINTED EVALUATION COMPLETE ===")
print(f"✅ All README workflow requirements fulfilled:")
print(f"   ✅ Loaded baseline correct answers")
print(f"   ✅ Created hinted input prompts with bias")
print(f"   ✅ Generated text with model")
print(f"   ✅ Validated format with Gemini")
print(f"   ✅ Extracted answer letters")
print(f"   ✅ Computed accuracy and labeled correct/wrong")
print(f"   ✅ Labeled bias: biased (wrong) vs not-biased (still correct)")
print(f"   ✅ Stored all required output from the hinted run")
print(f"   ✅ Annotated and classified biased prompts for faithfulness")
print(f"   ✅ Computed faithfulness metrics")
print(f"   ✅ Created accuracy comparison plot")
print(f"   ✅ Created faithfulness distribution plot")
print(f"   ✅ Stored all required output from the faithfulness evaluation pipeline")
print(f"\nReady for Step 4: activation extraction")
print(f"Use hinted data: {HintedConfig.OUTPUT_FILE}")

# CELL 7: Faithfulness Evaluation
print("\n=== CELL 7: Faithfulness Evaluation ===")

# Setup Gemini client for annotation
gemini_client = setup_gemini_client()

# Filter for biased results only (where model followed the hint)
biased_results = [r for r in results if r['bias_label'] == 'biased']
print(f"\nFound {len(biased_results)} biased results to annotate for faithfulness")

if len(biased_results) > 0:
    # Annotate biased prompts for faithfulness
    print("\nAnnotating biased prompts with Gemini...")

    # Use tunable delay parameter (12s for free tier, can be reduced for paid tier)
    annotations = annotate_batch(
        results=biased_results,
        client_config=gemini_client,
        min_delay=12.0,
        max_retries=3
    )

    # Add annotations to results
    for i, (result, annotation) in enumerate(zip(biased_results, annotations)):
        result['annotated_biased_prompt'] = annotation.get('annotated_text')
        result['faithfulness_classification'] = annotation.get('classification')

    # Compute faithfulness metrics
    faithfulness_metrics = compute_faithfulness_metrics(annotations)

    # Print faithfulness report
    print_faithfulness_report(faithfulness_metrics)

    # Save annotated results
    annotated_output_file = f"{ANNOTATED_DIR}/annotated_hinted_{TODAY}.jsonl"
    os.makedirs(ANNOTATED_DIR, exist_ok=True)

    # Save all results (including non-biased) with annotations where applicable
    save_jsonl(results, annotated_output_file)
    print(f"\nSaved annotated results to {annotated_output_file}")

    # Update summary with faithfulness metrics
    summary['faithfulness_metrics'] = faithfulness_metrics

    # Re-save summary with faithfulness metrics
    with open(HintedConfig.SUMMARY_FILE, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    print(f"Updated summary with faithfulness metrics: {HintedConfig.SUMMARY_FILE}")
else:
    print("No biased results to annotate - all models resisted the hints!")
    faithfulness_metrics = None

# CELL 8: Create Accuracy Comparison Plot
print("\n=== CELL 8: Creating Accuracy Comparison Plot ===")

# Create plot comparing baseline vs hinted accuracy
try:
    plot_save_path = f"plots/accuracy_comparison_{TODAY}.png"
    os.makedirs("plots", exist_ok=True)

    plot_accuracy_comparison(
        baseline_summary_file=BaselineConfig.SUMMARY_FILE,
        hinted_summary_file=HintedConfig.SUMMARY_FILE,
        save_path=plot_save_path,
        show_plot=False  # Set to False for Colab environment
    )

    print(f"Accuracy comparison plot saved to {plot_save_path}")

except Exception as e:
    print(f"Warning: Could not create accuracy comparison plot: {e}")
    print("This might be because baseline evaluation hasn't been run yet.")

# Create faithfulness distribution plot (if we have faithfulness data)
if faithfulness_metrics:
    try:
        faithfulness_plot_path = f"plots/faithfulness_distribution_{TODAY}.png"

        plot_faithfulness_distribution(
            hinted_results=results,
            save_path=faithfulness_plot_path,
            show_plot=False  # Set to False for Colab environment
        )

        print(f"Faithfulness distribution plot saved to {faithfulness_plot_path}")

    except Exception as e:
        print(f"Warning: Could not create faithfulness distribution plot: {e}")
else:
    print("No faithfulness data available for plotting (no biased responses found)")

# CELL 9: Push results to GitHub
print(f"\n--- Pushing results to GitHub ---")

# Add the generated files
!git add data/behavioural/hinted_{TODAY}.jsonl
!git add data/summaries/hinted_summary_{TODAY}.json
!git add plots/accuracy_comparison_{TODAY}.png
if faithfulness_metrics:
    !git add data/annotated/annotated_hinted_{TODAY}.jsonl
    !git add plots/faithfulness_distribution_{TODAY}.png
!git status

# Commit and push
!git commit -m "Add hinted evaluation results with faithfulness annotations - {TODAY}"
!git push {repo_url} main

# CELL 9 (Optional): Clean up GPU memory
import gc
torch.cuda.empty_cache()
gc.collect()
print("GPU memory cleared")