# -*- coding: utf-8 -*-
"""activations_test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qY-Fm10kFM3CNnOcKdRL9zlPdb0uMwpq
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install transformers torch accelerate torchlens scipy

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import torchlens as tl

# config to load the model
model_name = "deepseek-ai/DeepSeek-R1-Distill-Llama-8Bps"
hf_token = "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"
device = "cuda" if torch.cuda.is_available() else "cpu"

# loading tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    token=hf_token
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    token=hf_token,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

# model architecture
print(model)

# Using Teun's way:

#BlockOutputWrapper Class (core mechanism for both extracting and adding activations)

class BlockOutputWrapper(torch.nn.Module):
    def __init__(self, block):
        super().__init__()
        self.block = block
        self.last_hidden_state = None
        self.add_activations = None

    def forward(self, *args, **kwargs):
        output = self.block(*args, **kwargs)
        self.last_hidden_state = output[0]
        self.output_before_adding = output

        if self.add_activations is not None:
            output = (output[0] + self.add_activations,) + output[1:]

        self.output_after_adding = output
        return output

    def add(self, activations):
        self.add_activations = activations

    def reset(self):
        self.last_hidden_state = None
        self.add_activations = None

# Wrapping model layers with BlockOutputWrapper

layers_path = model.model.layers
wrapped_layers = torch.nn.ModuleList()
for i, layer in enumerate(layers_path):
    wrapped_layers.append(BlockOutputWrapper(layer))

model.model.layers = wrapped_layers

# config input and target layer

prompt = "The capital of Ireland is"
target_layer = model.model.layers[15]
inputs = tokenizer(prompt, return_tensors="pt").to(device)

# running the model for a baseline prediction

with torch.no_grad():
    outputs = model(**inputs)

# get the prediction from the final logits

last_token_logits = outputs.logits[0, -1, :]
predicted_token_id = torch.argmax(last_token_logits).item()
baseline_prediction = tokenizer.decode(predicted_token_id)

print(f"Baseline prediction:'{baseline_prediction}'")

# Extract the activations saved by the wrapper at our target layer.

last_token_activations = target_layer.last_hidden_state[0, -1, :]
print(f"\nExtracted activations from Layer {target_layer_idx}. Shape: {last_token_activations.shape}")

# Craft a random steering vector.
steering_vector = torch.randn_like(last_token_activations)
steering_strength = -2.5

# set steering hook around the target layer

target_layer.add(steering_vector * steering_strength)

# running the model for steered performance

with torch.no_grad():
    steered_outputs = model(**inputs)

# Get the new steered prediction. It will be some non-sense for sure!
steered_logits = steered_outputs.logits[0, -1, :]
steered_token_id = torch.argmax(steered_logits).item()
steered_prediction = tokenizer.decode(steered_token_id)
print(f"Steered prediction: '{steered_prediction}'")

# reset hooks
target_layer.reset()