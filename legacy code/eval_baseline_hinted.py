# -*- coding: utf-8 -*-
"""eval_baseline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14UlT--tATnRcoMWC6A2kr4mzzvWj-Hb5
"""

# Install the necessary libraries, including flash-attn for speed
!pip install transformers torch bitsandbytes accelerate flash-attn

# Log in to your Hugging Face account
# This model does not have a gated license, but logging in is good practice.
from huggingface_hub import login
login()

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# We've updated the model_id to point to the DeepSeek model.
model_id = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Load the model with the same optimizations as before
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    load_in_4bit=True,
    device_map="auto",
    torch_dtype=torch.bfloat16,
)

# Create the text-generation pipeline
generation_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=2048,
    do_sample=False,
    temperature=0.7,
    top_p=0.9,
)

# 1. Make sure you have uploaded your JSONL file to Colab.
jsonl_filename = '/content/mmlu_psychology_biased_prompts.jsonl'

# 2. Load the 'test' split by passing "test" as the argument.
test_prompts = load_prompts_by_split(jsonl_filename, split_to_load="validation")
print(f"Successfully loaded {len(test_prompts)} prompts from the 'test' split.")

# 3. Load the 'validation' split by passing "validation".
validation_prompts = load_prompts_by_split(jsonl_filename, split_to_load="validation")
print(f"Successfully loaded {len(validation_prompts)} prompts from the 'validation' split.")

import json

def load_prompts_by_split(filename, split_to_load, prompt_field):
    """
    Loads prompts from a specific field in a JSONL file for a specific data split.
    """
    prompts_list = []
    with open(filename, 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            if data.get("split") == split_to_load:
                if prompt_field in data:
                    prompts_list.append(data[prompt_field])
    return prompts_list

# --- How to use it ---
jsonl_filename = '/content/val_biased_prompts_2025-08-12.jsonl'
# Load the pre-formatted prompts from the 'biased_prompt' field
test_prompts = load_prompts_by_split(jsonl_filename, split_to_load="validation", prompt_field="biased_prompt")
print(f"Successfully loaded {len(test_prompts)} prompts from the 'validation' split.")

# ==============================================================================
# FINAL ROBUST GENERATION SCRIPT
# ==============================================================================

# 1. Imports
# ------------------------------------------------------------------------------
import json
import time
import torch
from tqdm import tqdm
import gc  # Import the garbage collection library

# This script assumes the following variables are already loaded in your notebook:
# - `model`: Your loaded language model.
# - `tokenizer`: Your loaded tokenizer.
# - `test_prompts`: A Python list of your pre-formatted prompt strings.
#   (e.g., loaded from the "biased_prompt" field of your JSONL file)


# 2. Setup
# ------------------------------------------------------------------------------
print("--- Starting Setup ---")

# Set a pad token if the tokenizer doesn't have one. This is crucial for batching.
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    print("Tokenizer pad_token set to eos_token.")

# Define the list of prompts you want the model to process.
prompts_to_run = test_prompts

# Set a safe, conservative batch size to prevent freezes.
batch_size = 5
print(f"Using safe batch size: {batch_size}")

# Initialize a list to store all generated answers.
all_answers = []


# 3. Processing Loop
# ------------------------------------------------------------------------------
print(f"\n--- Starting generation for {len(prompts_to_run)} prompts ---")
start_time = time.time()

for i in tqdm(range(0, len(prompts_to_run), batch_size), desc="Processing Batches"):

    # Create the current mini-batch of prompts.
    batch_prompts = prompts_to_run[i:i + batch_size]

    # Tokenize the batch, padding to the same length and truncating long prompts.
    inputs = tokenizer(
        batch_prompts,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=1024  # Truncate prompts longer than this to prevent errors.
    ).to(model.device)

    # Generate responses with no_grad to save memory.
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=1024,  # A safe limit for the generated response length.
            do_sample=False,     # Ensures deterministic (temperature=0) output.
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )

    # Decode the generated tokens, skipping the original prompt part.
    input_length = inputs['input_ids'].shape[1]
    batch_answers = tokenizer.batch_decode(outputs[:, input_length:], skip_special_tokens=True)

    # Add the newly generated answers to our main list.
    all_answers.extend(batch_answers)

    # --- Memory Cleanup Routine ---
    # This is critical to prevent gradual memory leaks and freezes.
    del inputs
    del outputs
    gc.collect()
    torch.cuda.empty_cache()

end_time = time.time()
print(f"\n--- Batch generation complete! ---")
print(f"Total time taken: {end_time - start_time:.2f} seconds")


# 4. Save Results
# ------------------------------------------------------------------------------
print("\n--- Saving results to file ---")
results_to_save = []
for i, answer in enumerate(all_answers):
    results_to_save.append({
        "prompt": prompts_to_run[i],
        "answer": answer.strip()
    })

output_filename = "val_baseline_results_2025-08-12.jsonl"
with open(output_filename, 'w', encoding='utf-8') as f:
    for result in results_to_save:
        f.write(json.dumps(result) + "\n")

print(f"Successfully saved {len(results_to_save)} results to {output_filename}")

