# -*- coding: utf-8 -*-
"""activation_extraction_tags.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16A6y7243yBOzy4Wk0jnzQdBTNXl3GfGC
"""

!pip install -U bitsandbytes

"""
activations_extraction_tags.py

Extracts hidden state activations from specified layers of a Hugging Face model.
Performs a "clean" forward pass by stripping all annotation tags from the
text before tokenization, using the tags only as pointers.
"""

import torch
import json
import os
import re
from tqdm import tqdm
import gc
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import argparse

# === 1. CONFIGURATION ===
# Set up command-line argument parsing for flexible use in the CLI
# parser = argparse.ArgumentParser(description="Extract sentence-level activations from a model on clean text.")
# parser.add_argument('--model_id', type=str, default="deepseek-ai/DeepSeek-R1-Distill-Llama-8B", help='Hugging Face model ID.')
# parser.add_argument('--jsonl_filename', type=str, required=True, help='Path to the JSONL file with annotated prompts.')
# parser.add_argument('--prompt_field', type=str, required=True, help='Field in the JSONL containing the full annotated text.')
# parser.add_argument('--output_dir', type=str, default="extracted_activations", help='Directory to save activation files.')
# args, unknown = parser.parse_known_args()

# --- Script Settings ---
model_id = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
jsonl_filename = "/content/gemini_annotated_val_biased_answers_mmlu_psy_2025-09-15.jsonl"
prompt_field = "annotated_biased_prompt"
output_dir = "sprint_2.2_sentences_activations_full_sweep_mmlu_psychology_val_2025-09-14"

# Define which annotation tags you want to extract activations for.
TARGET_TAGS = ["F", "F_wk", "U", "E", "N", "H", "Q", "A", "Fact", "F_final", "U_final"]
# Define the range of layers to extract from for a full sweep.
LAYERS_TO_EXTRACT = range(32)

# --- Setup ---
os.makedirs(output_dir, exist_ok=True)
print(f"--- Configuration ---\nModel: {model_id}\nInput File: {jsonl_filename}\nPrompt Field: {prompt_field}\nOutput Directory: {output_dir}\nTarget Tags: {TARGET_TAGS}\nLayers: All {len(LAYERS_TO_EXTRACT)}\n---------------------")

# Configure quantization
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16
)

# === 2. LOAD MODEL AND TOKENIZER ===
print(f"\n--- Loading model: {model_id} ---")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    quantization_config=quantization_config,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
model.eval()
print("--- Model and tokenizer loaded successfully. ---")

# === 3. WRAPPER CLASS AND MODEL INSTRUMENTATION ===
print("\n--- Preparing model for activation extraction... ---")
class BlockOutputWrapper(torch.nn.Module):
    """A wrapper to intercept and save the hidden states of a model layer."""
    def __init__(self, block):
        super().__init__()
        self.block = block
        self.last_hidden_state = None

    def forward(self, *args, **kwargs):
        output = self.block(*args, **kwargs)
        self.last_hidden_state = output[0]
        return output

    def reset(self):
        self.last_hidden_state = None

try:
    layers_path = model.model.layers
    for i, layer in enumerate(layers_path):
        layers_path[i] = BlockOutputWrapper(layer)
    print(f"Wrapped {len(layers_path)} layers successfully.")
except AttributeError:
    print(f"Error: Could not find 'model.model.layers' in {model_id}. Adjust the path.")
    exit()

# === 4. HELPER FUNCTIONS ===
def load_prompts_from_file(filename):
    """Loads a list of JSON objects from a JSONL file."""
    data_list = []
    with open(filename, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data_list.append(json.loads(line))
            except json.JSONDecodeError:
                print(f"Warning: Skipping malformed line in {filename}")
    return data_list

def get_clean_text_and_char_indices(annotated_text, target_tags):
    """
    Strips all [TAG] style annotations and calculates the new character
    positions of target periods in the clean text. Robust to extra spaces.
    """
    char_indices = {tag: [] for tag in target_tags}

    # 1. Find all tags (e.g., [H], [/Q]) to calculate character offsets.
    all_tags_pattern = re.compile(r'\[/?[\w_]+\]')
    tag_matches = list(all_tags_pattern.finditer(annotated_text))

    def get_offset(annotated_char_pos):
        # Calculates the total length of all tags preceding a character position.
        offset = 0
        for match in tag_matches:
            if match.start() < annotated_char_pos:
                offset += len(match.group(0))
            else:
                break
        return offset

    # 2. Find target periods in the annotated text and map their positions to the clean text.
    for tag in target_tags:
        # This regex allows for optional spaces (\s*) and captures the period in group 2.
        pattern = re.compile(fr'\[{tag}\](.*?)\s*(\.)\s*\[/{tag}\]')
        for match in pattern.finditer(annotated_text):
            # Get original position of the period itself.
            period_char_pos_annotated = match.start(2)
            # Calculate and subtract the offset of preceding tags.
            offset = get_offset(period_char_pos_annotated)
            period_char_pos_clean = period_char_pos_annotated - offset
            char_indices[tag].append(period_char_pos_clean)

    # 3. Create the clean text by removing all tags.
    clean_text = all_tags_pattern.sub('', annotated_text)

    return clean_text, char_indices

# === 5. MAIN EXTRACTION LOOP ===
print(f"\n--- Loading prompts from {jsonl_filename}... ---")
prompts_data = load_prompts_from_file(jsonl_filename)
print(f"Loaded {len(prompts_data)} prompts to process.")

for i, data_item in enumerate(tqdm(prompts_data, desc="Extracting Activations")):
    annotated_text = data_item.get(prompt_field)
    if not annotated_text:
        continue

    # 1. Generate clean text and get target character indices.
    clean_text, period_char_indices = get_clean_text_and_char_indices(annotated_text, TARGET_TAGS)

    if not any(period_char_indices.values()):
        continue

    # 2. Tokenize the CLEAN text.
    inputs = tokenizer(clean_text, return_tensors="pt").to(device)

    # Comprehensive debugging
    token_count = len(inputs['input_ids'][0])
    print(f"\n=== DEBUGGING PROMPT {i} ===")
    print(f"Clean text length: {len(clean_text)}")
    print(f"Token count: {token_count}")
    print(f"Clean text preview: '{clean_text[:200]}...'")

    # Show the tokenized sequence
    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
    print(f"First 10 tokens: {tokens[:10]}")
    print(f"Last 10 tokens: {tokens[-10:]}")

    # 3. Convert clean character indices to final token indices with detailed debugging.
    period_token_indices = {tag: [] for tag in TARGET_TAGS}
    for tag, char_indices_list in period_char_indices.items():
        print(f"\nProcessing tag '{tag}' with {len(char_indices_list)} character indices: {char_indices_list}")
        for char_idx in char_indices_list:
            # Show what character we're looking at
            if char_idx < len(clean_text):
                char_at_idx = clean_text[char_idx]
                print(f"  Char at index {char_idx}: '{char_at_idx}'")
            else:
                print(f"  ERROR: Char index {char_idx} is beyond clean text length {len(clean_text)}!")
                print(f"  This indicates a problem in get_clean_text_and_char_indices()")
                raise IndexError(f"Character index {char_idx} out of bounds for clean text of length {len(clean_text)}")

            token_idx = inputs.char_to_token(char_idx)
            print(f"  Token index for char {char_idx}: {token_idx}")

            if token_idx is not None:
                if token_idx >= token_count:
                    print(f"  ERROR: Token index {token_idx} >= token count {token_count}!")
                    print(f"  This indicates a tokenizer char_to_token mapping issue")
                    raise IndexError(f"Token index {token_idx} out of bounds for token sequence of length {token_count}")
                period_token_indices[tag].append(token_idx)
            else:
                print(f"  WARNING: char_to_token returned None for char index {char_idx}")

    print(f"\nFinal period token indices: {period_token_indices}")
    print("=== END DEBUGGING ===")

    # 4. Run the forward pass on the CLEAN text.
    with torch.no_grad():
        model(**inputs)

    prompt_activations = {layer_idx: {} for layer_idx in LAYERS_TO_EXTRACT}

    # 5. Extract activations for target tokens from each layer.
    for layer_idx in LAYERS_TO_EXTRACT:
        wrapped_layer = model.model.layers[layer_idx]
        hidden_state_output = wrapped_layer.last_hidden_state

        print(f"\nLayer {layer_idx} debug:")
        print(f"  Hidden state type: {type(hidden_state_output)}")
        print(f"  Hidden state shape: {hidden_state_output.shape if hasattr(hidden_state_output, 'shape') else 'No shape attr'}")

        # Based on your working script, the wrapper gives us a 2D tensor [seq_len, hidden_dim]
        full_activations_tensor = hidden_state_output

        for tag, indices in period_token_indices.items():
            if indices:
                # Store each sentence activation individually instead of concatenating
                individual_activations = []
                for idx in indices:
                    # At this point, all indices should be valid due to our earlier checks
                    single_activation = full_activations_tensor[idx, :].cpu()
                    individual_activations.append(single_activation)
                prompt_activations[layer_idx][tag] = individual_activations
        wrapped_layer.reset()

    # 6. Save the activations for the current prompt.
    save_path = os.path.join(output_dir, f"prompt_{i}_activations.pt")
    torch.save(prompt_activations, save_path)

    # Clean up memory after each prompt.
    gc.collect()
    torch.cuda.empty_cache()

print("\n--- Activation extraction co" \
"mplete! ---")

import shutil

# Create a zip file of the entire output directory
shutil.make_archive('/content/activations_archive','zip', output_dir)

# This creates /content/activations_archive.zip
print(f"Created archive: content/activations_archive.zip")
print(f"Archive contains {len(os.listdir(output_dir))} activation files")

from google.colab import files

files.download('/content/activations_archive.zip')